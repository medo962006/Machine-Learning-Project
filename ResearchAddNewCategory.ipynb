{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from random import shuffle\n",
    "import cv2\n",
    "model = tf.keras.models.load_model(\"model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_Path = \"\"\n",
    "new_category = \"\"\n",
    "categories=[new_category]\n",
    "new_training_data = []\n",
    "def training_data_initializer():\n",
    "    if new_dataset_Path != \"\":\n",
    "        for category in categories:\n",
    "            path = new_dataset_Path\n",
    "            Image_category = categories.index(category)\n",
    "            for img in listdir(path):\n",
    "                try:\n",
    "                    img_array = cv2.imread(f'{path}/{img}')\n",
    "                    new_array = cv2.resize(img_array , (32 , 32))\n",
    "                    if([new_array,Image_category] not in new_training_data):\n",
    "                        new_training_data.append([new_array , Image_category])\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "training_data_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "# Create a new input layer for the new dataset\n",
    "new_input = Input(shape=(new_dataset.shape[1],))\n",
    "\n",
    "# Concatenate the new input layer with the existing model\n",
    "concatenated_layers = concatenate([model.output, new_input])\n",
    "\n",
    "# Add a new dense layer to the model\n",
    "new_layer = Dense(64, activation='relu')(concatenated_layers)\n",
    "new_model = tf.keras.models.Model(inputs=[model.input, new_input], outputs=new_layer)\n",
    "# Compile the new model\n",
    "new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the new model with the new dataset\n",
    "new_model.fit([original_dataset, new_dataset], labels, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the new model with the new dataset\n",
    "new_model.evaluate([original_dataset, new_dataset], labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
